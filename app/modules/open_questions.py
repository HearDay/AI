from typing import List
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

from ..config import settings
from ..prompt_templates import build_open_question_prompt, conversational_rewrite

# ì „ì—­ ì´ˆê¸°í™” ì œê±°í•˜ê³ , ì§€ì—° ë¡œë”©ìœ¼ë¡œ ì „í™˜
_tokenizer = None
_model = None

def _load_llm():
    global _tokenizer, _model
    if _tokenizer is not None and _model is not None:
        return _tokenizer, _model
    print(f"ðŸ”¹ Loading model (lazy): {settings.HF_MODEL_ID}")
    _tokenizer = AutoTokenizer.from_pretrained(settings.HF_MODEL_ID, use_fast=True)
    _model = AutoModelForCausalLM.from_pretrained(
        settings.HF_MODEL_ID,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
        device_map=settings.DEVICE_MAP,
    )
    return _tokenizer, _model

def _apply_chat_template(system: str, user: str) -> str:
    tok, _ = _load_llm()
    if hasattr(tok, "apply_chat_template") and getattr(tok, "chat_template", None):
        messages = [{"role": "system", "content": system}, {"role": "user", "content": user}]
        return tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    return f"<<SYS>>\n{system}\n<</SYS>>\n\n[USER]\n{user}\n[/USER]\n[ASSISTANT] "

@torch.inference_mode()
def _generate(prompt: str) -> str:
    tok, mdl = _load_llm()
    inputs = tok(prompt, return_tensors="pt").to(mdl.device)
    output_ids = mdl.generate(
        **inputs,
        max_new_tokens=settings.HF_MAX_NEW_TOKENS,
        do_sample=True,
        temperature=settings.HF_TEMPERATURE,
        top_p=settings.HF_TOP_P,
        eos_token_id=tok.eos_token_id,
        pad_token_id=tok.eos_token_id,
    )
    text = tok.decode(output_ids[0], skip_special_tokens=True)
    # chat í…œí”Œë¦¿ ì‚¬ìš© ì‹œ prompt ì´í›„ê°€ ëª¨ë¸ ì‘ë‹µì¸ ê²½ìš°ê°€ ë§ŽìŒ
    return text.split(prompt, 1)[-1].strip()

def generate_open_questions(summary: str, level: str = "beginner") -> List[str]:
    """
    ë‰´ìŠ¤ ìš”ì•½ê³¼ ë ˆë²¨ì„ ë°›ì•„ ê°œë°©í˜• ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•´ ë°˜í™˜.
    FastAPI ì—”ë“œí¬ì¸íŠ¸ëŠ” main.pyì—ì„œ ì²˜ë¦¬í•œë‹¤.
    """
    system_prompt = "ë„ˆëŠ” ë‰´ìŠ¤ í† ë¡ ì„ ë•ëŠ” í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. ì‚¬ìš©ìžê°€ íŽ¸í•˜ê²Œ ëŠë¼ëŠ” ìžì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” í†¤ì„ ìœ ì§€í•œë‹¤."
    user_prompt = build_open_question_prompt(summary, level)
    full_prompt = _apply_chat_template(system_prompt, user_prompt)

    raw = _generate(full_prompt)
    raw = conversational_rewrite(raw)

    # ë¼ì¸ íŒŒì‹±(ë²ˆí˜¸ ë¶™ì€ í˜•ì‹ ìš°ì„ )
    lines = [l.strip() for l in raw.splitlines() if l.strip()]
    qs: List[str] = []
    for l in lines:
        # "1) ..." / "1." / "1ï¼‰..." ë“± ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤ ì»¤ë²„
        if (len(l) > 2 and l[:2].isdigit() and ")" in l):
            qs.append(l.split(")", 1)[-1].strip(" :"))
        elif l and l[0].isdigit() and (len(l) > 1 and l[1] in (")", ".", "ï¼‰")):
            qs.append(l[2:].strip(" :"))
        if len(qs) == 3:
            break

    # ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ì±„ìš°ê¸°(UXìš©)
    if len(qs) < 3:
        fallback = [
            "ì§€ê¸ˆ ì´ ì‚¬ì•ˆì—ì„œ ê°€ìž¥ í•µì‹¬ì ì¸ ì‚¬ì‹¤ì€ ë¬´ì—‡ì´ë¼ê³  ë³´ì‹œë‚˜ìš”?",
            "ì´ ê²°ì •ì´ ëˆ„êµ¬ì—ê²Œ ì–´ë–¤ ë³€í™”ë¥¼ ë§Œë“¤ ìˆ˜ ìžˆì„ê¹Œìš”?",
            "ë‹¹ì‹ ì´ë¼ë©´ ì–´ë–¤ ëŒ€ì•ˆì„ ë¨¼ì € ì‹œë„í•´ ë³´ì‹œê² ì–´ìš”?"
        ]
        qs = (qs + fallback)[:3]

    # ëë¬¸ìž¥ ë³´ì •(ë¬¼ìŒí‘œ ë³´ìž¥)
    qs = [q.rstrip("?.") + "?" if not q.endswith("?") else q for q in qs]
    return qs
