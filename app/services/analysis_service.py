import asyncio
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from fastapi.concurrency import run_in_threadpool
from app.models.document import Article, ArticleRecommend, ArticleRecommendVector, UserRecentArticle
import pickle

class AnalysisService:
    def __init__(self):
        """
        ì´ˆê¸°í™” ì‹œ ëª¨ë¸ ë¡œë”©ì€ í•˜ì§€ ì•Šê³ , ì¸ë±ìŠ¤ë§Œ ì¤€ë¹„
        ì‹¤ì œ ëª¨ë¸ ë¡œë”©ì€ load_and_build_index()ì—ì„œ ìˆ˜í–‰
        """
        print("AnalysisService ì´ˆê¸°í™” ì¤‘...")
        self.model = None
        self.d = 768
        self.index = faiss.IndexFlatIP(self.d)
        self.index_to_reco_id = {}
        self.index_lock = asyncio.Lock()  # Faiss ì¸ë±ìŠ¤ ì ‘ê·¼ ë™ê¸°í™”ìš©
        self.vector_id_to_article_id = {}

    async def _ensure_model_loaded(self):
        """ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¹„ë™ê¸°ë¡œ ë¡œë“œ"""
        if self.model is None:
            print("SBERT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            # run_in_threadpoolì— callableì„ ëª…ì‹œì ìœ¼ë¡œ ë„˜ê¹€
            def _load():
                return SentenceTransformer('jhgan/ko-sroberta-multitask')
            self.model = await run_in_threadpool(_load)

    async def encode_text(self, text: str) -> np.ndarray:
        """SBERT ì„ë² ë”©ì„ ë¹„ë™ê¸° ì‹¤í–‰ (ìŠ¤ë ˆë“œí’€ ì‚¬ìš©)"""
        await self._ensure_model_loaded()
        # model.encodeëŠ” ë¸”ë¡œí‚¹ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ threadpoolë¡œ ì‹¤í–‰
        embedding = await run_in_threadpool(self.model.encode, text)
        return np.asarray(embedding, dtype='float32')

    async def load_and_build_index(self, db: AsyncSession):
        """
        ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆ ì‹¤í–‰:
        - ëª¨ë“  ê¸°ì‚¬ ë²¡í„°ë¥¼ DBì—ì„œ ë¶ˆëŸ¬ì™€ FAISS ì¸ë±ìŠ¤ë¥¼ ë¹Œë“œ
        - index_to_reco_id, vector_id_to_article_id ë§¤í•‘ ìƒì„±
        """
        print("DBë¡œë¶€í„° Faiss ì¸ë±ìŠ¤ë¥¼ ë¹Œë“œí•©ë‹ˆë‹¤...")
        await self._ensure_model_loaded()

        # 1ï¸âƒ£ ëª¨ë“  ì¶”ì²œ ë²¡í„°ì™€ ID ê°€ì ¸ì˜¤ê¸°
        query = (
            select(
                ArticleRecommendVector.article_recommend_id,
                ArticleRecommendVector.sbert_vector
            )
            .join(ArticleRecommend, ArticleRecommendVector.article_recommend_id == ArticleRecommend.id)
            .join(Article, Article.article_recommend_id == ArticleRecommend.id)
        )
        result = await db.execute(query)
        rows = result.all()

        if not rows:
            print("ë¡œë“œí•  ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        import json

        all_vectors = []
        reco_ids = []
        for reco_id, vector_data in rows:
            if isinstance(vector_data, str):
                try:
                    vector_list = json.loads(vector_data)
                except json.JSONDecodeError:
                    print(f"[WARN] JSON íŒŒì‹± ì‹¤íŒ¨: reco_id={reco_id}")
                    continue
            else:
                vector_list = vector_data

            if isinstance(vector_list, (list, tuple)) and len(vector_list) == self.d:
                all_vectors.append(vector_list)
                reco_ids.append(reco_id)
            else:
                print(f"[WARN] ì˜ëª»ëœ ë²¡í„° ì°¨ì›: reco_id={reco_id}, len={len(vector_list) if vector_list else None}")

        if not all_vectors:
            print("ìœ íš¨í•œ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ ë¹Œë“œ ì¤‘ë‹¨.")
            return

        # 2ï¸âƒ£ numpy ë³€í™˜ ë° ì •ê·œí™”
        vectors_np = np.array(all_vectors, dtype='float32')
        async with self.index_lock:
            await run_in_threadpool(faiss.normalize_L2, vectors_np)

            start_idx = self.index.ntotal
            await run_in_threadpool(self.index.add, vectors_np)

            # 3ï¸âƒ£ reco_id ë§¤í•‘
            for i, reco_id in enumerate(reco_ids):
                self.index_to_reco_id[start_idx + i] = reco_id

            # 4ï¸âƒ£ article_id ë§¤í•‘
            article_ids_query = (
                select(Article.id, Article.article_recommend_id)
                .where(Article.article_recommend_id.in_(reco_ids))
            )
            article_result = await db.execute(article_ids_query)
            article_rows = article_result.all()
            reco_to_article = {reco_id: article_id for article_id, reco_id in article_rows}

            for i, reco_id in enumerate(reco_ids):
                article_id = reco_to_article.get(reco_id)
                if article_id:
                    self.vector_id_to_article_id[start_idx + i] = article_id

        print(f"ì´ {self.index.ntotal}ê°œì˜ ë²¡í„°ê°€ Faiss ì¸ë±ìŠ¤ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"vector_id_to_article_id ë§¤í•‘ ìˆ˜: {len(self.vector_id_to_article_id)}")

    async def add_document_to_index(self, reco_id: int, vector_list: list):
        """ìƒˆë¡œìš´ ê¸°ì‚¬ ì¶”ê°€ ì‹œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
        vector_np = np.array([vector_list], dtype='float32')

        async with self.index_lock:
            await run_in_threadpool(faiss.normalize_L2, vector_np)
            start_idx = self.index.ntotal
            await run_in_threadpool(self.index.add, vector_np)
            # ì—¬ëŸ¬ ë²¡í„° ì¶”ê°€ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ rangeë¡œ ì²˜ë¦¬ (ì—¬ê¸°ì„  1ê°œ)
            for i in range(vector_np.shape[0]):
                self.index_to_reco_id[start_idx + i] = reco_id
        print(f"ArticleRecommend ID {reco_id}ê°€ ì¸ë±ìŠ¤ {start_idx}ì— ì¶”ê°€ë¨")

    async def find_similar_documents_by_user(
        self, db: AsyncSession, user_id: int, top_k: int = 5
    ) -> list[int]:
        """
        [ê°œì¸í™” ì¶”ì²œ]
        ì‚¬ìš©ìê°€ ì½ì€ ì—¬ëŸ¬ ê¸°ì‚¬ë“¤ì˜ SBERT ë²¡í„° í‰ê· ì„ ê³„ì‚°í•˜ê³ ,
        ê·¸ í‰ê·  ë²¡í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Faissì—ì„œ ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰
        """
        print(f"\n=== [DEBUG] find_similar_documents_by_user(user_id={user_id}) ===")

        # 1ï¸âƒ£ ì‚¬ìš©ì ì½ì€ ê¸°ì‚¬ë“¤ì˜ ì¶”ì²œ ë²¡í„° ì¡°íšŒ
        query = (
            select(ArticleRecommendVector.sbert_vector)
            .join(ArticleRecommend, ArticleRecommendVector.article_recommend_id == ArticleRecommend.id)
            .join(Article, Article.article_recommend_id == ArticleRecommend.id)
            .join(UserRecentArticle, UserRecentArticle.article_id == Article.id)
            .where(UserRecentArticle.user_id == user_id)
        )
        result = await db.execute(query)
        user_vectors = result.scalars().all()

        import json

        # ë¬¸ìì—´ë¡œ ì €ì¥ëœ ë²¡í„°ë¥¼ íŒŒì‹± (TEXT ì»¬ëŸ¼ ëŒ€ì‘)
        parsed_vectors = []
        for v in user_vectors:
            if isinstance(v, str):
                try:
                    parsed_vectors.append(json.loads(v))
                except json.JSONDecodeError:
                    print(f"[WARN] ì˜ëª»ëœ JSON ë²¡í„° í˜•ì‹: {v[:80]}")
                    continue
            elif isinstance(v, (list, tuple)):
                parsed_vectors.append(v)
            else:
                print(f"[WARN] ì˜ˆìƒì¹˜ ëª»í•œ ë²¡í„° íƒ€ì…: {type(v)}")
        user_vectors = parsed_vectors

        # ë²¡í„° í˜•íƒœ ë””ë²„ê¹… ì¶œë ¥
        if user_vectors:
            print(f"[DEBUG] ë¶ˆëŸ¬ì˜¨ ë²¡í„° ì˜ˆì‹œ íƒ€ì…={type(user_vectors[0])}, ê¸¸ì´={len(user_vectors[0])}")
        else:
            print("[DEBUG] íŒŒì‹± í›„ ë²¡í„° ì—†ìŒ")

        if not user_vectors:
            print(f"[DEBUG] ì‚¬ìš©ì {user_id}ì˜ ì½ì€ ê¸°ì‚¬ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # 2ï¸âƒ£ numpy ë°°ì—´ë¡œ ë³€í™˜ + ì°¨ì› í™•ì¸
        try:
            user_arr = np.array(user_vectors, dtype='float32')
            if user_arr.ndim == 1:
                user_arr = user_arr.reshape(1, -1)
            elif user_arr.ndim != 2:
                raise ValueError(f"ë²¡í„° ì°¨ì› ì´ìƒ: ndim={user_arr.ndim}, ì˜ˆì‹œ={user_arr[:3]}")

            # ğŸš¨ ì˜ëª»ëœ ì°¨ì› ë°©ì–´
            if user_arr.shape[1] != self.index.d:
                print(f"[ê²½ê³ ] ì˜ëª»ëœ ë²¡í„° ì°¨ì› ë°œê²¬ ({user_arr.shape[1]} != {self.index.d}) â†’ í•„í„°ë§")
                # index.dì™€ ê°™ì€ ì°¨ì›ë§Œ ë‚¨ê¹€
                user_arr = np.array([v for v in user_vectors if len(v) == self.index.d], dtype='float32')
                if user_arr.size == 0:
                    print("[ERROR] ìœ íš¨í•œ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return []
        except Exception as e:
            print(f"[ERROR] numpy ë³€í™˜ ì‹¤íŒ¨: {repr(e)}")
            return []

        # 3ï¸âƒ£ í‰ê·  ë²¡í„° ê³„ì‚°
        user_profile = np.mean(user_arr, axis=0).reshape(1, -1)
        print(f"[DEBUG] user_profile shape={user_profile.shape}")

        # 4ï¸âƒ£ ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
        print(f"[DEBUG] index.ntotal={self.index.ntotal}, index.d={getattr(self.index, 'd', None)}")

        if getattr(self.index, "d", None) is None:
            print("[ERROR] ì¸ë±ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_and_build_index() ì‹¤í–‰ í•„ìš”.")
            return []

        # 5ï¸âƒ£ ì°¨ì› ë¶ˆì¼ì¹˜ í™•ì¸
        if user_profile.shape[1] != self.index.d:
            raise RuntimeError(
                f"[ì°¨ì› ë¶ˆì¼ì¹˜] user_profile={user_profile.shape[1]} / index.d={self.index.d} "
                f"â†’ DB ë²¡í„° ë˜ëŠ” ëª¨ë¸ ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜. ì¸ë±ìŠ¤ ì¬ìƒì„± í•„ìš”."
            )

        # 6ï¸âƒ£ ê²€ìƒ‰ (FaissëŠ” ë™ê¸°, threadpoolë¡œ ì‹¤í–‰)
        async with self.index_lock:
            await run_in_threadpool(faiss.normalize_L2, user_profile)
            num_search = top_k + len(user_vectors)
            D, I = await run_in_threadpool(self.index.search, user_profile, num_search)

        similar_reco_ids = []
        for faiss_index_id in I[0]:
            reco_id = self.index_to_reco_id.get(faiss_index_id)
            if reco_id:
                similar_reco_ids.append(reco_id)

        # 7ï¸âƒ£ ì´ë¯¸ ì½ì€ ê¸°ì‚¬ ì œì™¸
        read_reco_ids_query = (
            select(Article.article_recommend_id)
            .join(UserRecentArticle, UserRecentArticle.article_id == Article.id)
            .where(UserRecentArticle.user_id == user_id)
        )
        read_reco_ids_result = await db.execute(read_reco_ids_query)
        read_reco_ids = set(read_reco_ids_result.scalars().all())

        filtered_reco_ids = [rid for rid in similar_reco_ids if rid not in read_reco_ids]
        if not filtered_reco_ids:
            print(f"[DEBUG] ì¶”ì²œ ê°€ëŠ¥í•œ ìƒˆ ê¸°ì‚¬ ì—†ìŒ. (ëª¨ë‘ ì´ë¯¸ ì½ìŒ)")
            return []

        # 8ï¸âƒ£ ì¶”ì²œ ê¸°ì‚¬ ID ë°˜í™˜
        query_similar_articles = (
            select(Article.id)
            .where(Article.article_recommend_id.in_(filtered_reco_ids))
            .limit(top_k)
        )
        result = await db.execute(query_similar_articles)
        recommended_article_ids = result.scalars().all()

        print(f"[DEBUG] ì¶”ì²œëœ ê¸°ì‚¬ ID ëª©ë¡: {recommended_article_ids}\n")

        return recommended_article_ids

    async def find_similar_documents_by_article(
        self, db: AsyncSession, article_id: int, top_k: int = 5
    ) -> list[int]:
        """
        ì£¼ì–´ì§„ ê¸°ì‚¬(article_id)ì™€ ìœ ì‚¬í•œ ê¸°ì‚¬ë“¤ì„ Faiss ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ ì°¾ìŒ
        """
        # 1ï¸âƒ£ ê¸°ì¤€ ê¸°ì‚¬ ë²¡í„° ê°€ì ¸ì˜¤ê¸°
        query = (
            select(ArticleRecommendVector.sbert_vector)
            .join(ArticleRecommend, ArticleRecommendVector.article_recommend_id == ArticleRecommend.id)
            .join(Article, Article.article_recommend_id == ArticleRecommend.id)
            .where(Article.id == article_id)
        )
        result = await db.execute(query)
        vector_row = result.scalar_one_or_none()

        if not vector_row:
            raise ValueError(f"Article ID {article_id}ì˜ ë²¡í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if isinstance(vector_row, (bytes, bytearray)):
            query_vector = np.array(pickle.loads(vector_row)).astype("float32")
        else:
            query_vector = np.array(vector_row).astype("float32")

        # 2ï¸âƒ£ FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬ ë²¡í„° ê²€ìƒ‰
        D, I = self.index.search(np.array([query_vector]), top_k + 1)  # +1: ìê¸° ìì‹  í¬í•¨
        similar_indices = I[0][1:].tolist()  # ì²« ë²ˆì§¸(ìê¸° ìì‹ ) ì œì™¸

        # 3ï¸âƒ£ ID ë§¤í•‘ (ë²¡í„° ì¸ë±ìŠ¤ â†’ article_id)
        similar_article_ids = [
            self.vector_id_to_article_id[idx] for idx in similar_indices if idx in self.vector_id_to_article_id
        ]

        return similar_article_ids

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
analysis_service = AnalysisService()