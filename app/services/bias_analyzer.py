from transformers import pipeline
import torch

class BiasAnalyzer:
    def __init__(self):
        print("í¸í–¥ì„± ë¶„ì„ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        # í•œêµ­ì–´ NLI ëª¨ë¸ ì‚¬ìš©
        self.classifier = pipeline(
            "zero-shot-classification", 
            model="pongjin/roberta_with_kornli",
            device=0 if torch.cuda.is_available() or torch.backends.mps.is_available() else -1
        )
        print("í¸í–¥ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    # ğŸ‘‡ [ìˆ˜ì •ë¨] async def -> def (ë™ê¸° í•¨ìˆ˜ë¡œ ë³€ê²½)
    def analyze_bias(self, text: str) -> dict:
        """
        ê¸°ì‚¬ ë³¸ë¬¸ì„ ë¶„ì„í•˜ì—¬ 'NEUTRAL'(ì¤‘ë¦½) ë˜ëŠ” 'BIASED'(í¸í–¥) ë¼ë²¨ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        [ê¸´ê¸‰ ìˆ˜ì •] í•„í„°ë§ ê¸°ì¤€ ëŒ€í­ ì™„í™”
        - ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ê³¼ë„í•˜ê²Œ í•„í„°ë§ë˜ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ 'í¸í–¥' íŒì • ê¸°ì¤€ì„ ì—„ê²©í•˜ê²Œ ë†’ì…ë‹ˆë‹¤.
        """
        if not text:
            return {"label": "UNKNOWN", "score": 0.0}

        # 1. ë¼ë²¨ì„ 'ë‰´ìŠ¤' vs 'ê°œì¸ ì˜ê²¬'ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì¼ë°˜ ê¸°ì‚¬ê°€ ê±¸ëŸ¬ì§€ëŠ” ê²ƒ ë°©ì§€
        label_neutral = "ì‚¬ì‹¤ì„ ì „ë‹¬í•˜ëŠ” ë‰´ìŠ¤ ë³´ë„"
        label_biased = "ê¸€ì“´ì´ì˜ ì£¼ê´€ì ì¸ ì£¼ì¥ì´ ê°•í•œ ê¸€"
        
        candidate_labels = [label_neutral, label_biased]
        
        # ê°€ì„¤ í…œí”Œë¦¿
        hypothesis_template = "ì´ ê¸€ì€ {}ì…ë‹ˆë‹¤."

        short_text = text[:512]

        # AI ì¶”ë¡  ì‹¤í–‰ (await ì—†ì´ ë°”ë¡œ ì‹¤í–‰)
        try:
            result = self.classifier(
                short_text,
                candidate_labels,
                hypothesis_template=hypothesis_template,
                multi_label=False
            )
        except Exception as e:
            print(f"[BiasAnalyzer Error] ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ê°’(NEUTRAL) ë°˜í™˜: {e}")
            return {"label": "NEUTRAL", "score": 0.0}

        # ì ìˆ˜ ë§¤í•‘
        scores = {label: score for label, score in zip(result['labels'], result['scores'])}
        score_biased = scores.get(label_biased, 0.0)
        score_neutral = scores.get(label_neutral, 0.0)

        print(f"[Bias Debug] ì¤‘ë¦½({score_neutral:.4f}) vs í¸í–¥({score_biased:.4f})")

        # 2. ì„ê³„ê°’(Threshold)ì„ 0.85ë¡œ ìƒí–¥ (í™•ì‹¤í•œ ê²ƒë§Œ ê±°ë¦„)
        BIAS_THRESHOLD = 0.85

        if score_biased >= BIAS_THRESHOLD:
            # í¸í–¥ ì ìˆ˜ê°€ ì••ë„ì ìœ¼ë¡œ ë†’ì„ ë•Œë§Œ BIASED ë¦¬í„´
            return {"label": "BIASED", "score": score_biased}
        else:
            # ê·¸ ì™¸ì—ëŠ” ëª¨ë‘ NEUTRAL (ì•ˆì „í•˜ê²Œ í†µê³¼)
            final_score = score_neutral if score_neutral > score_biased else (1.0 - score_biased)
            return {"label": "NEUTRAL", "score": final_score}

# ì‹±ê¸€í„´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
bias_analyzer = BiasAnalyzer()